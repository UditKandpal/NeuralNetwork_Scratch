{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPuqNOWsaeF333IRXvw6mwO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UditKandpal/NeuralNetwork_Scratch/blob/main/NeuralNetwork_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neural Network Architecture from basic implementing Linear Equation Prediction --> y=2x"
      ],
      "metadata": {
        "id": "TzRU149LrdRd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4zYmdVxilWyh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Neural_Network:\n",
        "  def __init__(self,input_size, hidden_size, output_size):\n",
        "    # W1 SHAPE\n",
        "    self.W1 = np.random.randn(input_size,hidden_size) * 0.01\n",
        "    self.b1 = np.zeros((1,hidden_size))\n",
        "\n",
        "    # W2 SHAPE\n",
        "    self.W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
        "    self.b2 = np.zeros((1,output_size))\n",
        "\n",
        "  # ACTIVATION FUNCTION\n",
        "  def relu(self, Z):\n",
        "    return np.maximum(0,Z)\n",
        "\n",
        "  # RELU DERIVATIVE --> return 1 if Z>0, else 0\n",
        "  def relu_derivative(self,Z):\n",
        "    return (Z>0).astype(float)\n",
        "\n",
        "  # FORWARD PASS\n",
        "  def forward(self, X):\n",
        "    # input -> hidden layer\n",
        "    self.Z1 = np.dot(X, self.W1) + self.b1\n",
        "    self.A1 = self.relu(self.Z1)\n",
        "\n",
        "    # hidden -> output layer\n",
        "    self.Z2 = np.dot(self.A1,self.W2) + self.b2\n",
        "    y_pred = self.Z2\n",
        "    return y_pred\n",
        "\n",
        "\n",
        "  # BACKWARD PASS\n",
        "  def backward(self, X, y_act, y_pred, learning_rate):\n",
        "    m = X.shape[0]\n",
        "\n",
        "    # calculating the gradients\n",
        "    ## Loss Gradient\n",
        "    d_loss_output = 2 * (y_pred - y_act) / m\n",
        "\n",
        "    ## gradients for w2 and b2 ->\n",
        "    self.dW2 = np.dot(self.A1.T, d_loss_output)\n",
        "    self.db2 = np.sum(d_loss_output, axis=0, keepdims=True)\n",
        "\n",
        "    ## Propagating the error to the hidden layer\n",
        "    d_loss_hidden = np.dot(d_loss_output, self.W2.T)\n",
        "\n",
        "    ## Applying relu derivative\n",
        "    d_Z1 = d_loss_hidden * self.relu_derivative(self.Z1)\n",
        "\n",
        "    ## gradients for W1 and b1\n",
        "    self.dW1 = np.dot(X.T, d_Z1)\n",
        "    self.db1 = np.sum(d_Z1 , axis=0, keepdims=True)\n",
        "\n",
        "  # 6. Update Weights (Gradient Descent)\n",
        "    self.W1 -= learning_rate * self.dW1\n",
        "    self.b1 -= learning_rate * self.db1\n",
        "    self.W2 -= learning_rate * self.dW2\n",
        "    self.b2 -= learning_rate * self.db2\n",
        "\n",
        "  def train(self, X, y, epochs, learning_rate):\n",
        "          loss_history = []\n",
        "          for i in range(epochs):\n",
        "              # 1. Forward pass\n",
        "              y_pred = self.forward(X)\n",
        "\n",
        "              # 2. Calculate Loss (Mean Squared Error)\n",
        "              loss = np.mean((y_pred - y)**2)\n",
        "              loss_history.append(loss)\n",
        "\n",
        "              # 3. Backward pass (Update weights)\n",
        "              self.backward(X, y, y_pred, learning_rate)\n",
        "\n",
        "              if i % 100 == 0:\n",
        "                  print(f\"Epoch {i}, Loss: {loss:.6f}\")\n",
        "\n",
        "          return loss_history\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Create Dummy Data\n",
        "# Let's try to learn a simple pattern: y = 2x1 - 3x2 (Linear)\n",
        "# Or even simpler: Input is one number, Output is Input * 2\n",
        "X = np.array([[1], [2], [3], [4], [5]])  # Inputs\n",
        "y = np.array([[2], [4], [6], [8], [10]]) # Targets (y = 2x)\n",
        "\n",
        "# 2. Initialize Network\n",
        "# Input size 1 -> Hidden Neurons 5 -> Output size 1\n",
        "nn = Neural_Network(input_size=1, hidden_size=5, output_size=1)\n",
        "\n",
        "# 3. Train\n",
        "print(\"Starting Training...\")\n",
        "history = nn.train(X, y, epochs=1000, learning_rate=0.01)\n",
        "\n",
        "# 4. Predict on new data\n",
        "test_val = np.array([[6]]) # We expect 12\n",
        "prediction = nn.forward(test_val)\n",
        "\n",
        "print(\"\\n--- Result ---\")\n",
        "print(f\"Input: 6\")\n",
        "print(f\"Target: 12\")\n",
        "print(f\"Prediction: {prediction[0][0]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pq-c7iLBOgT0",
        "outputId": "c3fa13f4-a028-4548-a077-8c0fe1c80791"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Training...\n",
            "Epoch 0, Loss: 44.006524\n",
            "Epoch 100, Loss: 0.247433\n",
            "Epoch 200, Loss: 0.024617\n",
            "Epoch 300, Loss: 0.001663\n",
            "Epoch 400, Loss: 0.000098\n",
            "Epoch 500, Loss: 0.000006\n",
            "Epoch 600, Loss: 0.000000\n",
            "Epoch 700, Loss: 0.000000\n",
            "Epoch 800, Loss: 0.000000\n",
            "Epoch 900, Loss: 0.000000\n",
            "\n",
            "--- Result ---\n",
            "Input: 6\n",
            "Target: 12\n",
            "Prediction: 12.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementing the same network using the PyTorch"
      ],
      "metadata": {
        "id": "R9uZueSWrsKh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "## Defining the Architecture\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(1,5), # Input -> Hidden Layer\n",
        "    nn.ReLU(),   # Activation Function\n",
        "    nn.Linear(5,1) # Hidden -> Output Layer\n",
        ")\n",
        "\n",
        "## Defining Loss and Optimizer\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr = 0.01)\n",
        "\n"
      ],
      "metadata": {
        "id": "MH-ADZBpdi9G"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.tensor([[1.0],[2.0],[3.0],[4.0],[5.0]])\n",
        "y = torch.tensor([[2.0],[4.0],[6.0],[8.0],[10.0]])\n",
        "\n",
        "for epoch in range(1000):\n",
        "  y_pred = model(X)\n",
        "\n",
        "  #loss calculation\n",
        "  loss = criterion(y_pred, y)\n",
        "\n",
        "  # MAGIC\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if epoch%10==0:\n",
        "    print(f\"Epoch {epoch}, Loss = {loss.item():.4f} \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqEfI47utqXg",
        "outputId": "b75bef67-cb78-4452-b005-aa643d2b43e9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss = 0.0983 \n",
            "Epoch 10, Loss = 0.0818 \n",
            "Epoch 20, Loss = 0.0680 \n",
            "Epoch 30, Loss = 0.0565 \n",
            "Epoch 40, Loss = 0.0468 \n",
            "Epoch 50, Loss = 0.0388 \n",
            "Epoch 60, Loss = 0.0321 \n",
            "Epoch 70, Loss = 0.0265 \n",
            "Epoch 80, Loss = 0.0219 \n",
            "Epoch 90, Loss = 0.0181 \n",
            "Epoch 100, Loss = 0.0149 \n",
            "Epoch 110, Loss = 0.0123 \n",
            "Epoch 120, Loss = 0.0101 \n",
            "Epoch 130, Loss = 0.0083 \n",
            "Epoch 140, Loss = 0.0068 \n",
            "Epoch 150, Loss = 0.0056 \n",
            "Epoch 160, Loss = 0.0046 \n",
            "Epoch 170, Loss = 0.0038 \n",
            "Epoch 180, Loss = 0.0031 \n",
            "Epoch 190, Loss = 0.0025 \n",
            "Epoch 200, Loss = 0.0021 \n",
            "Epoch 210, Loss = 0.0017 \n",
            "Epoch 220, Loss = 0.0014 \n",
            "Epoch 230, Loss = 0.0011 \n",
            "Epoch 240, Loss = 0.0009 \n",
            "Epoch 250, Loss = 0.0008 \n",
            "Epoch 260, Loss = 0.0006 \n",
            "Epoch 270, Loss = 0.0005 \n",
            "Epoch 280, Loss = 0.0004 \n",
            "Epoch 290, Loss = 0.0003 \n",
            "Epoch 300, Loss = 0.0003 \n",
            "Epoch 310, Loss = 0.0002 \n",
            "Epoch 320, Loss = 0.0002 \n",
            "Epoch 330, Loss = 0.0001 \n",
            "Epoch 340, Loss = 0.0001 \n",
            "Epoch 350, Loss = 0.0001 \n",
            "Epoch 360, Loss = 0.0001 \n",
            "Epoch 370, Loss = 0.0001 \n",
            "Epoch 380, Loss = 0.0001 \n",
            "Epoch 390, Loss = 0.0000 \n",
            "Epoch 400, Loss = 0.0000 \n",
            "Epoch 410, Loss = 0.0000 \n",
            "Epoch 420, Loss = 0.0000 \n",
            "Epoch 430, Loss = 0.0000 \n",
            "Epoch 440, Loss = 0.0000 \n",
            "Epoch 450, Loss = 0.0000 \n",
            "Epoch 460, Loss = 0.0000 \n",
            "Epoch 470, Loss = 0.0000 \n",
            "Epoch 480, Loss = 0.0000 \n",
            "Epoch 490, Loss = 0.0000 \n",
            "Epoch 500, Loss = 0.0000 \n",
            "Epoch 510, Loss = 0.0000 \n",
            "Epoch 520, Loss = 0.0000 \n",
            "Epoch 530, Loss = 0.0000 \n",
            "Epoch 540, Loss = 0.0000 \n",
            "Epoch 550, Loss = 0.0000 \n",
            "Epoch 560, Loss = 0.0000 \n",
            "Epoch 570, Loss = 0.0000 \n",
            "Epoch 580, Loss = 0.0000 \n",
            "Epoch 590, Loss = 0.0000 \n",
            "Epoch 600, Loss = 0.0000 \n",
            "Epoch 610, Loss = 0.0000 \n",
            "Epoch 620, Loss = 0.0000 \n",
            "Epoch 630, Loss = 0.0000 \n",
            "Epoch 640, Loss = 0.0000 \n",
            "Epoch 650, Loss = 0.0000 \n",
            "Epoch 660, Loss = 0.0000 \n",
            "Epoch 670, Loss = 0.0000 \n",
            "Epoch 680, Loss = 0.0000 \n",
            "Epoch 690, Loss = 0.0000 \n",
            "Epoch 700, Loss = 0.0000 \n",
            "Epoch 710, Loss = 0.0000 \n",
            "Epoch 720, Loss = 0.0000 \n",
            "Epoch 730, Loss = 0.0000 \n",
            "Epoch 740, Loss = 0.0000 \n",
            "Epoch 750, Loss = 0.0000 \n",
            "Epoch 760, Loss = 0.0000 \n",
            "Epoch 770, Loss = 0.0000 \n",
            "Epoch 780, Loss = 0.0000 \n",
            "Epoch 790, Loss = 0.0000 \n",
            "Epoch 800, Loss = 0.0000 \n",
            "Epoch 810, Loss = 0.0000 \n",
            "Epoch 820, Loss = 0.0000 \n",
            "Epoch 830, Loss = 0.0000 \n",
            "Epoch 840, Loss = 0.0000 \n",
            "Epoch 850, Loss = 0.0000 \n",
            "Epoch 860, Loss = 0.0000 \n",
            "Epoch 870, Loss = 0.0000 \n",
            "Epoch 880, Loss = 0.0000 \n",
            "Epoch 890, Loss = 0.0000 \n",
            "Epoch 900, Loss = 0.0000 \n",
            "Epoch 910, Loss = 0.0000 \n",
            "Epoch 920, Loss = 0.0000 \n",
            "Epoch 930, Loss = 0.0000 \n",
            "Epoch 940, Loss = 0.0000 \n",
            "Epoch 950, Loss = 0.0000 \n",
            "Epoch 960, Loss = 0.0000 \n",
            "Epoch 970, Loss = 0.0000 \n",
            "Epoch 980, Loss = 0.0000 \n",
            "Epoch 990, Loss = 0.0000 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8MCG-HLLu_sS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}